import{_ as a}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as s,b as i,o as t}from"./app-DImQijK3.js";const n="/assets/java-inflater1-Bch7KN_d.png",o="/assets/java-inflater2-WWlRgyHm.png",c={};function l(d,e){return t(),s("div",null,e[0]||(e[0]=[i(`<h1 id="一次线上oom问题排查" tabindex="-1"><a class="header-anchor" href="#一次线上oom问题排查"><span>一次线上OOM问题排查</span></a></h1><p>生产环境的JVM进程经常被运维报告有OOM的情况，运维的描述是，内存一直在缓慢增长，1-2天就会出现OOM的情况。因为已经严重影响到客户的使用，所以采取由运维定时监控，与客户交流，开发负责排查问题的策略。</p><h2 id="step1-测试环境复现问题" tabindex="-1"><a class="header-anchor" href="#step1-测试环境复现问题"><span>Step1：测试环境复现问题</span></a></h2><p>由于开发是没权限进入生产环境的，要高效率解决问题，必须能在测试环境复现。查看生产环境的日志，确认容器出发OOM的接口，然后使用<code>Jmeter</code>在测试环境压测该接口，发生OOM的情况，问题能够复现。</p><h2 id="step2-在测试环境复现" tabindex="-1"><a class="header-anchor" href="#step2-在测试环境复现"><span>Step2: 在测试环境复现</span></a></h2><p>在测试环境建一个和生产环境规格一样的容器，使用<code>Jmeter</code>压测，确认问题能复现。</p><h2 id="step3-初步确认问题原因" tabindex="-1"><a class="header-anchor" href="#step3-初步确认问题原因"><span>Step3: 初步确认问题原因</span></a></h2><p>查看Java应用日志，没有OOM信息，使用<code>dmesg</code>查看系统日志，确认是因为OOM而被操作系统杀掉。</p><div class="language-text line-numbers-mode" data-highlighter="shiki" data-ext="text" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-text"><span class="line"><span>kernel: [1799319.246494] Out of memory: Kill process 28536 (java) score 673 or sacrifice childSep </span></span>
<span class="line"><span>kernel: [1799319.246506] Killed process 28536 (java) total-vm:1271568kB, anon-rss:426528kB, file-rss:0kB</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="step4-在压测过程中观察指标" tabindex="-1"><a class="header-anchor" href="#step4-在压测过程中观察指标"><span>Step4: 在压测过程中观察指标</span></a></h2><ul><li><code>top</code>命令观察内存占用情况</li><li><code>arthas</code>观察JVM的内存情况</li><li><code>jcmd</code>观察JVM的内存情况</li></ul><p>容器内存为<code>8GB</code>，堆内存设置为<code>5.6GB</code>开始第一次观察。</p><h3 id="第一次观察" tabindex="-1"><a class="header-anchor" href="#第一次观察"><span>第一次观察</span></a></h3><p>压测前和压测过程中已经在内存快爆之前使用<code>jcmd</code>观察内存情况，发现total的<code>committed</code>和<code>reserved</code>大小只有小幅度的上升，而<code>top</code>里观察到的<code>RES</code>值一直在上升，直至内存爆掉。</p><p>而total committed的值为7.2GB，非常接近容器的8GB，未压测前虽然committed 的total差不多为7G，但是这些内存未被实际使用，所以没有被映射到物理内存里，所以开始时<code>RES</code>值比committed低很多，随着压测进行，越来越多内存被交换到物理内存里，最后容器内存超过8G被killed。</p><p>由于堆外内存使用量大，堆内存不能配置为容器内存的70%，改为4GB，容器内存的50%值再压力测试。</p><h3 id="第二次观察" tabindex="-1"><a class="header-anchor" href="#第二次观察"><span>第二次观察</span></a></h3><p>堆内存改为<code>4GB</code>后，进行一个小时的压测，容器不再被killed，JVM也未OOM。但此时还有一个问题，top观察到的物理内存RES占用比jcmd里的committed高很多，也就是说，jcmd追踪不到的堆外内存占用多，需要排查是什么占用了这部分内存。</p><p>使用<code>pmap</code>观察进程内存的变化，发现压测前和压测后比对，<code>pmap</code>里多了大约<code>100</code>个<code>15M</code>左右的<code>anon</code>内存块。</p><p>使用smaps获取可疑的<code>15M</code>内存块的起始地址和结束地址，之后使用<code>gdb</code>dump出内存块。</p><p>启动gdb</p><p><code>gdb -p &lt;pid&gt;</code></p><p>dump指定内存地址到指定的目录下，参数的地址需要在smaps拿到地址前加上0x。</p><p><code>dump memory /tmp/0x7fb9b0000000-0x7fb9b3ffe000.dump 0x7fb9b0000000 0x7fb9b3ffe000</code></p><p>显示长度超过10字符的字符串。或者把内存块下载到本地，使用<code>vscode</code>之类的文本编辑器打开分析，或者使用二进制编辑器分析二进制。也可以使用<code>string</code>查看字符串内容。</p><p><code>strings -10 /tmp/0x7fb9b0000000-0x7fb9b3ffe000.dump</code></p><p>发现内存块中有大量的jar包里的<code>MANIFEST.MF</code>文件的字符串，猜测是代码里有加载jar包到堆外内存里。</p><h3 id="第三次观察" tabindex="-1"><a class="header-anchor" href="#第三次观察"><span>第三次观察</span></a></h3><p>在压测过程中观察JVM的线程栈。</p><p>注意到压测大部分线程的栈都在以下RUNNABLE中，</p><div class="language-text line-numbers-mode" data-highlighter="shiki" data-ext="text" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-text"><span class="line"><span>at java.util.zip.Inflater.inflateBytes(Native Method)</span></span>
<span class="line"><span>at java.util.zip.Inflater.inflate(Inflater.java:259)</span></span>
<span class="line"><span>- locked &lt;0x00000007720e8bf0&gt; (a java.util.zip.ZStreamRef)</span></span>
<span class="line"><span>at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:152)</span></span>
<span class="line"><span>at sun.misc.IOUtils.readFully(IOUtils.java:163)</span></span>
<span class="line"><span>at java.util.jar.JarFile.getBytes(JarFile.java:452)</span></span>
<span class="line"><span>at java.util.jar.JarFile.getManifestFromReference(JarFile.java:196)</span></span>
<span class="line"><span>at java.util.jar.JarFile.getManifest(JarFile.java:184)</span></span>
<span class="line"><span>at com.sun.tools.javac.file.FSInfo.getJarClassPath(FSInfo.java:69)</span></span>
<span class="line"><span>at com.sun.tools.javac.file.Locations$Path.addJarClassPath(Locations.java:305)</span></span>
<span class="line"><span>at com.sun.tools.javac.file.Locations$Path.addFile(Locations.java:296)</span></span>
<span class="line"><span>at com.sun.tools.javac.file.Locations$Path.addFiles(Locations.java:236)</span></span>
<span class="line"><span>at com.sun.tools.javac.file.Locations$Path.addFiles(Locations.java:242)</span></span>
<span class="line"><span>at com.sun.tools.javac.file.Locations$SimpleLocationHandler.setLocation(Locations.java:439)</span></span>
<span class="line"><span>at com.sun.tools.javac.file.Locations.setLocation(Locations.java:697)</span></span>
<span class="line"><span>at com.sun.tools.javac.file.JavacFileManager.setLocation(JavacFileManager.java:798)</span></span>
<span class="line"><span>at com.xxx.xxx.JavaCompileService.compile(JavaCompileService.java:86)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>业务代码这个接口会使用<code>JavaCompiler</code>动态编译代码，又由于<code>at java.util.zip.Inflater.inflateBytes(Native Method)</code>这里是使用堆外内存加载zip文件，猜测就是这里代码把jar包加在到堆外内存中。</p><h3 id="分析inflater和inflaterinputstream" tabindex="-1"><a class="header-anchor" href="#分析inflater和inflaterinputstream"><span>分析Inflater和InflaterInputStream</span></a></h3><p>分析<code>InflaterInputStream</code>中的<code>close</code>方法，发现其在<code>close</code>时不一定会释放内存，而是依赖于<code>Inflater</code>类被回收时，调用<code>finalize</code>方法时释放内存。</p><figure><img src="`+n+'" alt="Alt text" tabindex="0" loading="lazy"><figcaption>Alt text</figcaption></figure><figure><img src="'+o+'" alt="Alt text" tabindex="0" loading="lazy"><figcaption>Alt text</figcaption></figure><h3 id="观察inflater在堆内存的数量" tabindex="-1"><a class="header-anchor" href="#观察inflater在堆内存的数量"><span>观察Inflater在堆内存的数量</span></a></h3><p>在压测前，压测后，手动gc后分别观察<code>java.util.zip.Inflater</code>对象的数量。</p><div class="language-shell line-numbers-mode" data-highlighter="shiki" data-ext="shell" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-shell"><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">jmap</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;"> -histo</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> &lt;</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">pi</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">d&gt; | </span><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">grep</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;"> java.util.zip.Inflater</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>观察堆里的对象数</p><div class="language-shell line-numbers-mode" data-highlighter="shiki" data-ext="shell" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-shell"><span class="line"><span style="--shiki-light:#4078F2;--shiki-dark:#61AFEF;">jcmd</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> &lt;</span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">pi</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">d&gt; </span><span style="--shiki-light:#50A14F;--shiki-dark:#98C379;">GC.run</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>手动执行GC发现压测后确实比压测前多了很多<code>java.util.zip.Inflater</code>对象，手动gc后减少了很多，但是从top里观察内存并没有减少占用。</p><h3 id="内存碎片分析" tabindex="-1"><a class="header-anchor" href="#内存碎片分析"><span>内存碎片分析</span></a></h3><p>考虑到是内存碎片的原因，使用<code>tcmalloc</code>代替Linux默认的内存分配器<code>ptmalloc</code>。</p><p>使用<code>tcmalloc</code>压测后，top里的内存占用为<code>5.8G</code>，jcmd里的内存committed total为<code>5.5G</code>。 对比使用<code>ptmalloc</code>压测后，top里的内存占用为<code>6.8G</code>，jcmd里的内存committed total为<code>5.5G</code>。 再使用<code>pmap</code>观察进程内存分布，发现之前存在的大量<code>15MB</code>左右的内存块已经没了。推断为大概率是内存碎片导致内存释放后，内存分配器没有把内存还给操作系统。</p><p>使用<code>jemalloc</code>压测过程中，top里的内存占用为<code>5.8G</code>，和<code>tcmalloc</code>基本一致，不过<code>jemalloc</code>内存有浮动，在<code>5.6-6G</code>之间浮动，但大多数时间保持在<code>5.8G</code>。 然而<code>jemalloc</code>在压测完后，手动执行GC之后，内存降到<code>5.4G</code>，这里应该就是因为<code>java.util.zip.Inflater</code>被回收后释放的内存，而<code>tcmalloc</code>和<code>ptmalloc</code>在手动GC后，内存占用不会产生变化。从<code>pmap</code>里观察堆内存的<code>RSS</code>值和手动GC前一致，所以该减少不是因为堆GC的原因。所以推导在该场景下<code>jemalloc</code>比<code>tcmalloc</code>内存碎片整理更好。</p><p>结论为：<code>jemalloc</code> &gt; <code>tcmalloc</code> &gt;&gt; <code>ptmalloc</code>。</p><h2 id="后记" tabindex="-1"><a class="header-anchor" href="#后记"><span>后记</span></a></h2><p>之后发现可以通过gdb调用<code>malloc_stats</code>来检查glibc缓存的内存大小，调用<code>malloc_trim(0)</code>释放缓存的内存。也可以通过<code>tcmalloc</code>或者<code>jemalloc</code>检测是否有内存泄漏。这样排查会更方便。</p>',49)]))}const h=a(c,[["render",l]]),m=JSON.parse('{"path":"/posts/java-oom-record.html","title":"一次线上OOM问题排查","lang":"zh-CN","frontmatter":{"category":["Java","Linux"],"tag":["OOM","线上问题"],"date":"2023-11-02T00:00:00.000Z","star":true,"description":"一次线上OOM问题排查 生产环境的JVM进程经常被运维报告有OOM的情况，运维的描述是，内存一直在缓慢增长，1-2天就会出现OOM的情况。因为已经严重影响到客户的使用，所以采取由运维定时监控，与客户交流，开发负责排查问题的策略。 Step1：测试环境复现问题 由于开发是没权限进入生产环境的，要高效率解决问题，必须能在测试环境复现。查看生产环境的日志，确...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"一次线上OOM问题排查\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2023-11-02T00:00:00.000Z\\",\\"dateModified\\":\\"2023-11-19T15:06:54.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"coldgust\\",\\"url\\":\\"https://github.com/coldgust\\",\\"email\\":\\"zhengxiaojian@apache.org\\"}]}"],["meta",{"property":"og:url","content":"https://coldgust.github.io/posts/java-oom-record.html"}],["meta",{"property":"og:site_name","content":"coldgust"}],["meta",{"property":"og:title","content":"一次线上OOM问题排查"}],["meta",{"property":"og:description","content":"一次线上OOM问题排查 生产环境的JVM进程经常被运维报告有OOM的情况，运维的描述是，内存一直在缓慢增长，1-2天就会出现OOM的情况。因为已经严重影响到客户的使用，所以采取由运维定时监控，与客户交流，开发负责排查问题的策略。 Step1：测试环境复现问题 由于开发是没权限进入生产环境的，要高效率解决问题，必须能在测试环境复现。查看生产环境的日志，确..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2023-11-19T15:06:54.000Z"}],["meta",{"property":"article:tag","content":"线上问题"}],["meta",{"property":"article:tag","content":"OOM"}],["meta",{"property":"article:published_time","content":"2023-11-02T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2023-11-19T15:06:54.000Z"}]]},"git":{"createdTime":1698854835000,"updatedTime":1700406414000,"contributors":[{"name":"Xiaojian Zheng","username":"","email":"zhengxiaojian@apache.org","commits":3}]},"readingTime":{"minutes":10.59,"words":1588},"filePathRelative":"posts/java-oom-record.md","excerpt":"\\n<p>生产环境的JVM进程经常被运维报告有OOM的情况，运维的描述是，内存一直在缓慢增长，1-2天就会出现OOM的情况。因为已经严重影响到客户的使用，所以采取由运维定时监控，与客户交流，开发负责排查问题的策略。</p>\\n<h2>Step1：测试环境复现问题</h2>\\n<p>由于开发是没权限进入生产环境的，要高效率解决问题，必须能在测试环境复现。查看生产环境的日志，确认容器出发OOM的接口，然后使用<code>Jmeter</code>在测试环境压测该接口，发生OOM的情况，问题能够复现。</p>\\n<h2>Step2: 在测试环境复现</h2>\\n<p>在测试环境建一个和生产环境规格一样的容器，使用<code>Jmeter</code>压测，确认问题能复现。</p>","autoDesc":true}');export{h as comp,m as data};
